{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing backpropagation (in practice)\n",
    "\n",
    "We've {ref}`already seen vectorized computational graphs in action <chapter:tensors/mlp>`, where we used layers of the form\n",
    "\n",
    "$$\n",
    "\\tanh \\big( X A \\big), \\quad X \\in \\mathbb{R}^{N \\times n}, \\quad A \\in \\mathbb{R}^{n \\times m},\n",
    "$$\n",
    "\n",
    "where $ X \\in \\mathbb{R}^{N \\times n} $ is the data matrix, and $ A \\in \\mathbb{R}^{n \\times m} $ is the parameter matrix. In other words: vectorization is not enough; we need to *tensorize*. (There's no such term as tensorize; I just made that one up.)\n",
    "\n",
    "Let's look at matrix addition: for any $ X, Y \\in \\mathbb{R}^{n \\times m} $, their sum $ Z = X + Y \\in \\mathbb{R}^{n \\times m} $ defines the familiar V-shaped graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"322pt\" height=\"131pt\"\n",
       " viewBox=\"0.00 0.00 321.70 131.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 127)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-127 317.7,-127 317.7,4 -4,4\"/>\n",
       "<!-- X -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-105\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- Z -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Z</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"63\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Z</text>\n",
       "</g>\n",
       "<!-- X&#45;&gt;Z -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>X&#45;&gt;Z</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M34.11,-87.21C39.22,-75.14 46.21,-58.64 52.03,-44.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"55.35,-46.04 56.02,-35.47 48.9,-43.31 55.35,-46.04\"/>\n",
       "</g>\n",
       "<!-- X  -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>X </title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"182\" cy=\"-105\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"182\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">dZ/dX</text>\n",
       "</g>\n",
       "<!-- Z  -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>Z </title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"229\" cy=\"-18\" rx=\"38.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"229\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">dZ/dZ</text>\n",
       "</g>\n",
       "<!-- X &#45;&gt;Z  -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>X &#45;&gt;Z </title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M191.29,-87.21C197.98,-75.1 207.14,-58.53 214.76,-44.76\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"217.96,-46.2 219.73,-35.76 211.83,-42.82 217.96,-46.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"228\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">∂Z/∂X</text>\n",
       "</g>\n",
       "<!-- Y -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Y</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-105\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">Y</text>\n",
       "</g>\n",
       "<!-- Y&#45;&gt;Z -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>Y&#45;&gt;Z</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M91.89,-87.21C86.78,-75.14 79.79,-58.64 73.97,-44.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"77.1,-43.31 69.98,-35.47 70.65,-46.04 77.1,-43.31\"/>\n",
       "</g>\n",
       "<!-- Y  -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>Y </title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"276\" cy=\"-105\" rx=\"37.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"276\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">dZ/dY</text>\n",
       "</g>\n",
       "<!-- Y &#45;&gt;Z  -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>Y &#45;&gt;Z </title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M268.55,-87.05C264.06,-77.23 258.06,-64.73 252,-54 250.14,-50.7 248.08,-47.3 245.99,-43.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"248.84,-41.94 240.46,-35.45 242.97,-45.75 248.84,-41.94\"/>\n",
       "<text text-anchor=\"middle\" x=\"278.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">∂Z/∂Y</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7d92e4298170>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "digraph = Digraph()\n",
    "\n",
    "nodes = [\"X\", \"Y\", \"Z\"]\n",
    "edges = [(\"X\", \"Z\"), (\"Y\", \"Z\")]\n",
    "offset = \" \"\n",
    "\n",
    "for v in nodes:\n",
    "    digraph.node(v)\n",
    "    digraph.node(v + offset, f\"dZ/d{v}\")\n",
    "\n",
    "for u, v in edges:\n",
    "    digraph.edge(u, v)\n",
    "    digraph.edge(u + offset, v + offset, f\"∂{v}/∂{u}\")\n",
    "\n",
    "digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do $ \\frac{dX}{dY} $ and $ \\frac{\\partial X}{\\partial Y} $ mean?\n",
    "\n",
    "Uh-oh. We have a snag. Following the logic of the Jacobian, these should be four-dimensional *tensors*:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{dX}{dY} &= \\bigg( \\frac{dx_{i,j}}{dy_{k, l}} \\bigg)_{i, j, k, l = 1, \\dots, n} \\in \\mathbb{R}^{n \\times n \\times n \\times n}, \\\\\n",
    "\\frac{\\partial X}{\\partial Y} &= \\bigg( \\frac{\\partial x_{i,j}}{\\partial y_{k, l}} \\bigg)_{i, j, k, l = 1, \\dots, n} \\in \\mathbb{R}^{n \\times n \\times n \\times n}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "How do we multiply tensors?\n",
    "\n",
    "We don't. Rather than spending blood, sweat, and tears to figure out how multidimensional matrices (a.k.a. tensors) are multiplied, we circumvent the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational graphs in practice\n",
    "\n",
    "Fortunately, we can completely avoid working with matrix-matrix derivatives by noticing that the terminal node of our computational graphs in training is always a scalar: the loss function!\n",
    "\n",
    "Take a look at multivariate linear regression:\n",
    "\n",
    "$$\n",
    "    \\mathbf{p} = X A,\n",
    "$$\n",
    "\n",
    "where $ X \\in \\mathbb{R}^{N \\times n} $ represents the data, $ A \\in \\mathbb{R}^{n \\times m} $ encodes the parameters, and $ \\mathbf{p} \\in \\mathbb{R}^{N \\times m} $ contains the predictions. (Note that we have omitted the bias for simplicity; you can assume that the bias is included in $ A $ and $ X $ is augmented with a row of ones.)\n",
    "\n",
    "With these in mind, the loss is defined by\n",
    "\n",
    "$$\n",
    "l = \\frac{1}{N} \\mathrm{sum}\\Big( (\\mathbf{y} - \\mathbf{p})^2 \\Big),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $ \\mathbf{y} \\in \\mathbb{R}^{N \\times m} $ is the ground truth,\n",
    "* $ \\mathbf{y} - \\mathbf{p} $ is the *pointwise difference* of the two matrices,\n",
    "* $ (\\mathbf{y} - \\mathbf{p})^2 $ is the pointwise square of $ \\mathbf{y} - \\mathbf{p} $,\n",
    "* and $ \\mathrm{sum} \\Big( (\\mathbf{y} - \\mathbf{p})^2 \\Big) $ is the sum of the elements of $ (\\mathbf{y} - \\mathbf{p})^2 $.\n",
    "\n",
    "All in all, $ l $ is a scalar! Here's the computational graph and its derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"368pt\" height=\"218pt\"\n",
       " viewBox=\"0.00 0.00 367.80 218.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 214)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-214 363.8,-214 363.8,4 -4,4\"/>\n",
       "<!-- X -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-192\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- 𝐩 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>𝐩</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"63\" cy=\"-105\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">𝐩</text>\n",
       "</g>\n",
       "<!-- X&#45;&gt;𝐩 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>X&#45;&gt;𝐩</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M34.11,-174.21C39.22,-162.14 46.21,-145.64 52.03,-131.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"55.35,-133.04 56.02,-122.47 48.9,-130.31 55.35,-133.04\"/>\n",
       "</g>\n",
       "<!-- X  -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>X </title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"178\" cy=\"-192\" rx=\"34.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"178\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">dl/dX</text>\n",
       "</g>\n",
       "<!-- 𝐩  -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>𝐩 </title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"240\" cy=\"-105\" rx=\"34.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"240\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">dl/d𝐩</text>\n",
       "</g>\n",
       "<!-- X &#45;&gt;𝐩  -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>X &#45;&gt;𝐩 </title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M186.93,-174.25C192.52,-164.28 200.14,-151.55 208,-141 210.98,-137 214.34,-132.94 217.74,-129.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"220.51,-131.21 224.64,-121.45 215.33,-126.5 220.51,-131.21\"/>\n",
       "<text text-anchor=\"middle\" x=\"228\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">∂𝐩/∂X</text>\n",
       "</g>\n",
       "<!-- A -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>A</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-192\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">A</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;𝐩 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>A&#45;&gt;𝐩</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M91.89,-174.21C86.78,-162.14 79.79,-145.64 73.97,-131.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"77.1,-130.31 69.98,-122.47 70.65,-133.04 77.1,-130.31\"/>\n",
       "</g>\n",
       "<!-- A  -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>A </title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"265\" cy=\"-192\" rx=\"34.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"265\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">dl/dA</text>\n",
       "</g>\n",
       "<!-- A &#45;&gt;𝐩  -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>A &#45;&gt;𝐩 </title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M259.94,-173.8C256.46,-161.97 251.77,-146.03 247.82,-132.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"251.15,-131.49 244.97,-122.89 244.43,-133.47 251.15,-131.49\"/>\n",
       "<text text-anchor=\"middle\" x=\"274\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">∂𝐩/∂A</text>\n",
       "</g>\n",
       "<!-- l -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>l</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"123\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"123\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">l</text>\n",
       "</g>\n",
       "<!-- 𝐩&#45;&gt;l -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>𝐩&#45;&gt;l</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M74,-88.41C82.97,-75.71 95.78,-57.56 106.05,-43.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.16,-44.68 112.06,-34.49 103.44,-40.64 109.16,-44.68\"/>\n",
       "</g>\n",
       "<!-- l  -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>l </title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"297\" cy=\"-18\" rx=\"31.4\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"297\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">dl/dl</text>\n",
       "</g>\n",
       "<!-- 𝐩 &#45;&gt;l  -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>𝐩 &#45;&gt;l </title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M249.71,-87.57C255.66,-77.7 263.54,-64.98 271,-54 273.44,-50.41 276.09,-46.67 278.74,-43.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"281.67,-44.95 284.81,-34.83 276.05,-40.79 281.67,-44.95\"/>\n",
       "<text text-anchor=\"middle\" x=\"288\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">∂l/∂𝐩</text>\n",
       "</g>\n",
       "<!-- 𝐲 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>𝐲</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"135\" cy=\"-105\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"135\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">𝐲</text>\n",
       "</g>\n",
       "<!-- 𝐲&#45;&gt;l -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>𝐲&#45;&gt;l</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M132.57,-86.8C130.93,-75.16 128.72,-59.55 126.85,-46.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"130.29,-45.59 125.42,-36.18 123.36,-46.57 130.29,-45.59\"/>\n",
       "</g>\n",
       "<!-- 𝐲  -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>𝐲 </title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"326\" cy=\"-105\" rx=\"33.6\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"326\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">dl/d𝐲</text>\n",
       "</g>\n",
       "<!-- 𝐲 &#45;&gt;l  -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>𝐲 &#45;&gt;l </title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M320.27,-87.21C316.22,-75.33 310.7,-59.17 306.06,-45.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"309.26,-44.09 302.72,-35.76 302.63,-46.35 309.26,-44.09\"/>\n",
       "<text text-anchor=\"middle\" x=\"329.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">∂l/∂𝐲</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7d92e429bb60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digraph = Digraph()\n",
    "\n",
    "nodes = [\"X\", \"A\", \"𝐩\", \"𝐲\", \"l\"]\n",
    "edges = [(\"X\", \"𝐩\"), (\"A\", \"𝐩\"), (\"𝐩\", \"l\"), (\"𝐲\", \"l\")]\n",
    "offset = \" \"\n",
    "\n",
    "for v in nodes:\n",
    "    digraph.node(v)\n",
    "    digraph.node(v + offset, f\"dl/d{v}\")\n",
    "\n",
    "for u, v in edges:\n",
    "    digraph.edge(u, v)\n",
    "    digraph.edge(u + offset, v + offset, f\"∂{v}/∂{u}\")\n",
    "\n",
    "\n",
    "digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the global derivatives: as the loss $ l \\in \\mathbb{R} $ is a scalar, global derivatives are all matrices: $ \\frac{dl}{dX} \\in \\mathbb{R}^{N \\times n} $, $ \\frac{dl}{dA} \\in \\mathbb{R}^{n \\times m} $, $ \\frac{dl}{d\\mathbf{p}} \\in \\mathbb{R}^{n \\times 1} $, $ \\frac{dl}{d\\mathbf{y}} \\in \\mathbb{R}^{n \\times 1} $, and $ \\frac{dl}{dl} \\in \\mathbb{R} $.\n",
    "\n",
    "In other words, if the terminal node is a scalar, the global derivative of a node has the same shape as the node itself.\n",
    "\n",
    "We cut off one head of the dragon, but what about the other? The local derivatives are still out of control: $ \\frac{\\partial \\mathbf{p}}{\\partial X} \\in \\mathbb{R}^{N \\times N \\times n} $ and $ \\frac{\\partial \\mathbf{p}}{\\partial X} \\in \\mathbb{R}^{N \\times n \\times m} $. Let's look at what happens along the single edge $ X \\to \\mathbf{p} $. Hell, let's even write this out componentwise:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{1, 1} & x_{1, 2} & \\dots & x_{1, n} \\\\\n",
    "x_{2, 1} & x_{2, 2} & \\dots & x_{2, n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{N, 1} & x_{N, 2} & \\dots & x_{N, n} \\\\\n",
    "\\end{bmatrix} \\mapsto \\begin{bmatrix}\n",
    "p_{1, 1} & p_{1, 2} & \\dots & p_{1, m} \\\\\n",
    "p_{2, 1} & p_{2, 2} & \\dots & p_{2, m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "p_{N, 1} & p_{N, 2} & \\dots & p_{N, m} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "You can think about a $ p_{i, j} $ as the function of the elements of $ X $. We don't explicitly indicate this, but it's important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During backpropagation, we take a step in the opposite direction, mapping the global derivative $ \\frac{dl}{d\\mathbf{p}} $ to $ \\frac{dl}{dX} $:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{dl}{dp_{1, 1}} & \\frac{dl}{dp_{1, 2}} & \\dots & \\frac{dl}{dp_{1, m}} \\\\\n",
    "\\frac{dl}{dp_{2, 1}} & \\frac{dl}{dp_{2, 2}} & \\dots & \\frac{dl}{dp_{2, m}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{dl}{dp_{N, 1}} & \\frac{dl}{dp_{N, 2}} & \\dots & \\frac{dl}{dp_{N, m}} \\\\\n",
    "\\end{bmatrix} \\mapsto \\begin{bmatrix}\n",
    "\\frac{dl}{dx_{1, 1}} & \\frac{dl}{dx_{1, 2}} & \\dots & \\frac{dl}{dx_{1, n}} \\\\\n",
    "\\frac{dl}{dx_{2, 1}} & \\frac{dl}{dx_{2, 2}} & \\dots & \\frac{dl}{dx_{2, n}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{dl}{dx_{N, 1}} & \\frac{dl}{dx_{N, 2}} & \\dots & \\frac{dl}{dx_{N, n}} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "I know. That's a lot of derivatives. With the scalar version of the chain rule, we can express every single element of $ \\frac{dy}{dX} $ by\n",
    "\n",
    "$$\n",
    "\\frac{dl}{dx_{i, j}} = \\sum_{k = 1}^{N} \\sum_{l = 1}^{m} \\frac{dl}{dp_{k, l}} \\frac{\\partial p_{k, l}}{\\partial x_{i, j}}.\n",
    "$$ (eq:tensors/vectorizing-backpropagation/full-chain-rule)\n",
    "\n",
    "Again, that's a lot of indices. Think of $ i $ and $ j $ as fixed while $ k $ and $ l $ run through all possible values.\n",
    "\n",
    "Bad news: we won't always be able to describe the backward step with a compact linear algebraic expression. Good news: in practice, most $ \\frac{\\partial p_{k, l}}{\\partial x_{i, j}} $ values are zero! So, {eq}`eq:tensors/vectorizing-backpropagation/full-chain-rule` will always be simplified.\n",
    "\n",
    "We'll walk through all the operations: addition, multiplication, broadcasting, reshaping, and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic operations\n",
    "\n",
    "Unlike `Scalar`s, we'll implement the backward step differently for each operation. How can we solve this?\n",
    "\n",
    "Our plan is to\n",
    "* enhance the `Edge` class to handle the custom backward functions,\n",
    "* and update the `_backward_step` to call that function.\n",
    "\n",
    "Let's go. We'll do the `Edge`. first. As it is a `namedtuple`, this is as simple as adding a new `backward_fn` attribute, in which we'll store a callable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "Edge = namedtuple(\n",
    "    \"Edge\", [\"prev\", \"local_grad\", \"backward_fn\"], defaults=[None, None, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(We've added `None` as default values. This will be useful later.)\n",
    "\n",
    "`backward_fn` gives us infinite freedom in our implementation, so let's agree only to use functions that take three arguments: \n",
    "* `tensor`, the `Tensor` from which the backward step is called,\n",
    "* `local_grad`, the local gradient of the step,\n",
    "* and `prev`, the `Tensor` whose backward gradient is being calculated.\n",
    "\n",
    "Here's what it looks like in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def typical_backward_fn(tensor, local_grad, prev):\n",
    "    \"\"\"\n",
    "    Computes the backward gradient for the given tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor : Tensor\n",
    "        The tensor from which the backward step is performed.\n",
    "    local_grad : Tensor\n",
    "        The local gradient at the current layer.\n",
    "    prev : Tensor\n",
    "        The tensor to which the backward gradient will be propagated.\n",
    "    \"\"\"\n",
    "    \n",
    "    return 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the `Tensor`. This is the first time we are looking into its internals. Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        value: np.ndarray,\n",
    "        prevs: List = None,\n",
    "    ):\n",
    "        self.value = np.array(value)\n",
    "        self.prevs = prevs if prevs is not None else []\n",
    "        self.backwards_grad = np.zeros_like(value)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.value.__repr__().replace(\"array\", \"Tensor\")\n",
    "\n",
    "    def _backward_step(self):\n",
    "        for prev, local_grad, backward_fn in self.prevs:\n",
    "            prev.backwards_grad += backward_fn(self, local_grad, prev)\n",
    "\n",
    "    def _get_graph(self, zero_grad=False):\n",
    "        ordered_tensors = []\n",
    "        visited_tensors = set()\n",
    "\n",
    "        def traverse_graph(x):\n",
    "            if x not in visited_tensors:\n",
    "                visited_tensors.add(x)\n",
    "\n",
    "                if zero_grad:\n",
    "                    x.backwards_grad = 0\n",
    "\n",
    "                for prev, _, _ in x.prevs:\n",
    "                    traverse_graph(prev)\n",
    "\n",
    "                ordered_tensors.append(x)\n",
    "\n",
    "        traverse_graph(self)\n",
    "\n",
    "        return ordered_tensors\n",
    "\n",
    "    def _zero_grad(self):\n",
    "        self._get_graph(zero_grad=True)\n",
    "\n",
    "    def gradient_update(self, lr):\n",
    "        self.value -= lr * self.backwards_grad\n",
    "\n",
    "    def backward(self, zero_grad=True):\n",
    "        ordered_tensors = self._get_graph(zero_grad=zero_grad)\n",
    "\n",
    "        self.backwards_grad = np.ones_like(self.value)\n",
    "\n",
    "        for tensor in reversed(ordered_tensors):\n",
    "            tensor._backward_step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `Tensor.value` is a NumPy array. (Unlike `Scalar.value`, a vanilla number type.) Also, check out `_backward_step`: instead of incrementing by the product of `backwards_grad` and `local_grad,` we call the `backward_fn.`\n",
    "\n",
    "Let's look at an example so we can actually understand this. As usual, we start with the simplest one: the addition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition\n",
    "\n",
    "We've seen this thousands of times: for any two matrices $ X, Y \\in \\mathbb{R}^{n \\times m} $, their sum is defined by\n",
    "\n",
    "$$\n",
    "Z = X + Y = \\begin{bmatrix}\n",
    "x_{1, 1} + y_{1, 1} & x_{1, 2} + y_{1, 2} & \\dots & x_{1, m} + y_{1, m} \\\\\n",
    "x_{2, 1} + y_{2, 1} & x_{2, 2} + y_{2, 2} & \\dots & x_{1, m} + y_{1, m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n, 1} + y_{n, 1} & x_{n, 2} + y_{n, 2} & \\dots & x_{n, m} + y_{n, m} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Matrices are added elementwise. Nothing special about that. \n",
    "\n",
    "Suppose there's a loss $ l \\in \\mathbb{R} $ down the line, and we are launching our backpropagation from that node. We can calculate the partial derivatives $ \\frac{\\partial z_{k, l}}{\\partial x_{i, j}} $ easily. The expression {eq}`eq:tensors/vectorizing-backpropagation/full-chain-rule` might scare you, but most terms are zeros:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{dl}{dx_{i, j}} &= \\sum_{k = 1}^{N} \\sum_{l = 1}^{m} \\frac{dl}{dz_{k, l}} \\frac{\\partial z_{k, l}}{\\partial x_{i, j}} = \\frac{dl}{dz_{i, j}},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "as because of $ z_{k, l} = x_{k, l} + y_{k, l} $, \n",
    "\n",
    "$$\n",
    "\n",
    "\\frac{\\partial z_{k, l}}{\\partial x_{i, j}} = \\begin{cases}\n",
    "1 & \\text{if } i = k \\text{ and } l = j, \\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Similarly, $ \\frac{dl}{dy_{i, j}} = \\frac{dl}{dz_{i, j}} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we implement this? In a sense, we can \"collapse\" the four-dimensional tensor that is the local derivative into a matrix of ones; while the backwards function simply multiplies the backwards gradient elementwise with it. (Or in other words, it does nothing.) Let's call it `_pointwise`, which you can find it at `mlfz.nn.tensor.utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pointwise(tensor, local_grad, prev):\n",
    "    \"\"\"\n",
    "    Accumulation of the backwards gradient via pointwise multiplication.\n",
    "    \"\"\"\n",
    "    \n",
    "    return tensor.backwards_grad * local_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see that `Tensor.__add__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(Tensor):\n",
    "    # ...\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Pointwise addition of tensors.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "\n",
    "        return Tensor(\n",
    "            value=self.value + other.value,\n",
    "            prevs=[\n",
    "                Edge(prev=self, local_grad=np.ones_like(self), backward_fn=_pointwise),\n",
    "                Edge(prev=other, local_grad=np.ones_like(other), backward_fn=_pointwise),\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I know, we could have defined a backward function that simply copies the backward gradient, but I decided to go with `_pointwise`. We'll reuse this in a hot minute.)\n",
    "\n",
    "Let's test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Tensor(2*np.ones((3, 4)))\n",
    "Y = Tensor(3*np.ones((3, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = X + Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[5., 5., 5., 5.],\n",
       "       [5., 5., 5., 5.],\n",
       "       [5., 5., 5., 5.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the backwards gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 1.0, 1.0, 1.0],\n",
       "       [1.0, 1.0, 1.0, 1.0],\n",
       "       [1.0, 1.0, 1.0, 1.0]], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.backwards_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 1.0, 1.0, 1.0],\n",
       "       [1.0, 1.0, 1.0, 1.0],\n",
       "       [1.0, 1.0, 1.0, 1.0]], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.backwards_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is what we expected. Awesome! Let's move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise multiplication\n",
    "\n",
    "Alright, one more warm-up example. For any two matrices $ X, Y \\in \\mathbb{R}^{n \\times m} $, we define their pointwise product by\n",
    "\n",
    "$$\n",
    "Z = X \\odot Y = \\begin{bmatrix}\n",
    "x_{1, 1} y_{1, 1} & x_{1, 2} y_{1, 2} & \\dots & x_{1, m} y_{1, m} \\\\\n",
    "x_{2, 1} y_{2, 1} & x_{2, 2} y_{2, 2} & \\dots & x_{2, m} y_{2, m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n, 1} y_{n, 1} & x_{n, 2} y_{n, 2} & \\dots & x_{n, m} y_{n, m} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Officially, $ \\odot $ is called the Hadamard product, but we'll call it elementwise multiplication. Sorry, Hadamard. I would love it if you could take a shot at implementing this by yourself. Pick up a pen and a piece of paper, compute the global backward gradient elementwise with the chain rule (like we did for multiplication), and fill in the blanks of `Tensor.__mul__` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(Tensor):\n",
    "    # ...\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        pass\n",
    "    \n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready? Here we go.\n",
    "\n",
    "As $ z_{i, j} = x_{i, j} y_{i, j} $, we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_{i, j}}{\\partial x_{k, l}} = \\begin{cases}\n",
    "y_{i, j} & \\text{if } k = i \\text{ and } l = j, \\\\\n",
    "0 & \\text{otherwise},\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_{i, j}}{\\partial y_{k, l}} = \\begin{cases}\n",
    "x_{i, j} & \\text{if } k = i \\text{ and } l = j, \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "There are lots of indices right here (as is often the case with tensors), but it's really simple: the expression $ z_{i, j} = x_{i, j} y_{i, j} $ only depends on $ x_{i, j} $ and $ y_{i, j} $, and nothing else. Thus, given the loss value $ l $ down the line, substituting into {eq}`eq:tensors/vectorizing-backpropagation/full-chain-rule` yields the global gradient\n",
    "\n",
    "$$\n",
    "\\frac{dl}{dX} = \\frac{dl}{dZ} \\odot Y, \\quad \\frac{dl}{dY} = \\frac{dl}{dZ} \\odot X.\n",
    "$$\n",
    "\n",
    "So, here's `Tensor.__mul__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(Tensor):\n",
    "    # ...\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        \"\"\"\n",
    "        Pointwise multiplication of tensors.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "\n",
    "        return Tensor(\n",
    "            value=self.value + other.value,\n",
    "            prevs=[\n",
    "                Edge(prev=self, local_grad=other.value, backward_fn=_pointwise),\n",
    "                Edge(prev=other, local_grad=self.value, backward_fn=_pointwise),\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Tensor(2*np.ones((3, 4)))\n",
    "Y = Tensor(3*np.ones((3, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = X * Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[5., 5., 5., 5.],\n",
       "       [5., 5., 5., 5.],\n",
       "       [5., 5., 5., 5.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the backwards step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the backwards gradients correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(X.backwards_grad == Y.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(Y.backwards_grad == X.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elementwise division works similarly; I'll leave that to you as an exercise. Let's jump into the meaty parts and talk about matrix multiplication!\n",
    "\n",
    "### Matrix multiplication\n",
    "\n",
    "Now, things start to get interesting. For any two matrices $ X \\in \\mathbb{R}^{p \\times q} $ and $ Y \\in \\mathbb{R}^{q \\times r} $ (watch the dimensions!), their product is defined by\n",
    "\n",
    "$$\n",
    "Z = X Y = \\Big( \\sum_{k = 1}^{q} x_{i, k} y_{k, j} \\Big)_{i, j = 1}^{p, r} \\in \\mathbb{R}^{p \\times r},\n",
    "$$\n",
    "\n",
    "that is, $ z_{i, j} = \\sum_{k = 1}^{q} x_{i, k} y_{k, j} $.\n",
    "\n",
    "Brace yourselves: complex formulas incoming.\n",
    "\n",
    "As before, the chain formula implies that\n",
    "\n",
    "$$\n",
    "\\frac{dl}{dx_{i, j}} = \\sum_{k = 1}^{p} \\sum_{l = 1}^{r} \\frac{dl}{dz_{k, l}} \\frac{\\partial z_{k, l}}{\\partial x_{i, j}}.\n",
    "$$\n",
    "\n",
    "Due to how $ z_{k, l} $ is defined, the local derivatives are\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial z_{k, l}}{\\partial x_{i, j}} &= \\frac{\\partial}{ \\partial x_{i, j} } \\Big(\\sum_{a = 1}^{p} x_{k, a} y_{a, l} \\Big) \\\\\n",
    "&= \\begin{cases}\n",
    "y_{j, l} & \\text{if } i = k, \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{dl}{dx_{i, j}} &= \\sum_{k = 1}^{p} \\sum_{l = 1}^{r} \\frac{dl}{dz_{k, l}} \\frac{\\partial z_{k, l}}{\\partial x_{i, j}} \\\\\n",
    "&= \\sum_{l = 1}^{r} \\frac{dl}{dz_{i, l}} y_{j, l}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If you stare at this above expression long enough, you'll notice that it looks a lot like the element of a product matrix, with a single flaw: the second term features $ y_{j, l} $, where $ l $ is the running index. However, we can remedy this by noticing that $ y_{i, l} $ is the $ (l, i) $-th element of the transpose $ Y^T $! Hence,\n",
    "\n",
    "$$\n",
    "\\frac{dl}{dX} = \\frac{dl}{dZ} Y^T,\n",
    "$$\n",
    "\n",
    "and similarly,\n",
    "\n",
    "$$\n",
    "\\frac{dl}{dY} = X^T \\frac{dl}{dZ}.\n",
    "$$\n",
    "\n",
    "(I won't walk you through the second one; you should do that as an exercise.) Thus, in a sense, the local gradient of matrix multiplication is $ Y^T $ and $ X^T $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{prf:remark} Dimension analysis.\n",
    "\n",
    "One way to remember the formulas\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{dl}{dX} &= \\frac{dl}{dZ} Y^T, \\\\\n",
    "\\frac{dl}{dY} &= X^T \\frac{dl}{dZ}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "is to perform *dimension analysis*; that is, check the dimensions of the matrices and figure out the arrangement by matching the dimensions. For instance, as $ \\frac{dl}{dX} \\in \\mathbb{R}^{p \\times q} $ and the formula involves the product of $ \\frac{dl}{dZ} \\in \\mathbb{R}^{p \\times r} $ and $ Y \\in \\mathbb{R}^{q \\times r} $, the configuration $ \\frac{dl}{dZ} Y^T $ is the only way to make the formula work.\n",
    "\n",
    "Although dimension analysis is not an exact mathematical tool, it can be useful if you have to figure things out quickly and you don't want to lose yourself among the multitude of indices. It sure did help me a lot.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk code. In Python, matrix multiplication is represented by the `@` operator, implemented via the `__matmul__` method. With our already established framework, it will be a walk in the park. The hard part was figuring out the formulas.\n",
    "\n",
    "First, we need to define the appropriate backward functions similar to `_pointwise` for addition and pointwise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _matmul_left(tensor, local_grad, prev):\n",
    "    \"\"\"\n",
    "    Accumulation of the backwards gradient via matrix multiplication.\n",
    "    \"\"\"\n",
    "    \n",
    "    return tensor.backwards_grad @ local_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _matmul_right(tensor, local_grad, prev):\n",
    "    \"\"\"\n",
    "    Accumulation of the backwards gradient via matrix multiplication.\n",
    "    \"\"\"\n",
    "    \n",
    "    return local_grad @ tensor.backwards_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `Tensor.__matmul__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(Tensor):\n",
    "    # ...\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        \"\"\"\n",
    "        Multiplication of tensors.\n",
    "        \"\"\"\n",
    "              \n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "\n",
    "        return Tensor(\n",
    "            value=self.value @ other.value,\n",
    "            prevs=[\n",
    "                Edge(prev=self, local_grad=other.value.T, backward_fn=_matmul_left),\n",
    "                Edge(prev=other, local_grad=self.value.T, backward_fn=_matmul_right),\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if the implementation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Tensor(np.array([\n",
    "    [1, 1, 1],\n",
    "    [2, 2, 2]\n",
    "]))\n",
    "\n",
    "Y = Tensor(np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = X @ Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[15, 18, 21, 24],\n",
       "       [30, 36, 42, 48]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(X.backwards_grad == Z.backwards_grad @ Y.value.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(Y.backwards_grad == X.value.T @ Z.backwards_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backprop works as intended! With that, we are done with matrix multiplication and the essential binary operations. Sure, we haven't spelled out pointwise division (`__truediv__`) and pointwise exponentiation (`__exp__`), but these are left for you as an exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposition\n",
    "\n",
    "Is that all? Are we done with tensors?\n",
    "\n",
    "Hardly. There are a few quirks that are expected from tensors. Take a look at the computation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1, 1, 1],\n",
    "    [2, 2, 2]\n",
    "])\n",
    "\n",
    "Y = np.array([\n",
    "    [1],\n",
    "    [2]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2],\n",
       "       [4, 4, 4]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X + Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically speaking, $ X $ is a $ 2 \\times 3 $ matrix, while $ Y $ is $ 2 \\times 1 $. Yet, in NumPy, their sum is properly defined by the following rule:\n",
    "* blow up $ Y $ along the \"second axis\" to match the dimensions (this is called *broadcasting*),\n",
    "* then apply elementwise addition.\n",
    "\n",
    "Currently, our `Tensor` does not support this. Check out the following computation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Tensor(np.array([\n",
    "    [1, 1, 1],\n",
    "    [2, 2, 2]\n",
    "]))\n",
    "\n",
    "Y = Tensor(np.array([\n",
    "    [1],\n",
    "    [2]\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = X + Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 1]], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.backwards_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code runs, but is it correct? No. The shape of `Y.backwards_grad` must match the shape of `Y`. This is not the case. Something must be wrong.\n",
    "\n",
    "I'll tell you what: broadcasting is omitted from our current computational graphs, so we are missing a step! To solve this, we need to look at the unary operations that will serve as our building blocks. (Unary operation means a single-variable operation. I know it sounds clunky, but that's what it is called.) There are four of them:\n",
    "* transposition,\n",
    "* reshaping,\n",
    "* broadcasting,\n",
    "* and summation.\n",
    "\n",
    "Let's start with transposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any matrix $ X \\in \\mathbb{R}^{n \\times m} $, its transpose $ X^T \\in \\mathbb{R}^{m \\times n} $ is simply defined by \"flipping the axes\", that is,\n",
    "\n",
    "$$\n",
    "X^T = (x_{j, i})_{i, j = 1}^{m, n}.\n",
    "$$\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6\n",
    "\\end{bmatrix}^T = \\begin{bmatrix}\n",
    "1 & 4 \\\\\n",
    "2 & 5 \\\\\n",
    "3 & 6 \n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "NumPy even supports it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4],\n",
       "       [2, 5],\n",
       "       [3, 6]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our `Tensor` class support transposition, we must first find its gradient. Trust me when I say this: it is much harder to put it into words than to figure it out. If $ x_{i, j}^T = x_{j, i} $ denotes the $ (i, j) $-th element of $ X^T $, then according to the chain rule,\n",
    "\n",
    "$$\n",
    "\\frac{dl}{dx_{i, j}} = \\sum_{k = 1}^{m} \\sum_{l = 1}^{n} \\frac{dl}{d x_{k, l}^T} \\frac{\\partial x_{k, l}^T}{\\partial x_{i, j}}.\n",
    "$$\n",
    "\n",
    "As\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{k, l}^T}{\\partial x_{i, j}} = \\begin{cases}\n",
    "1 & \\text{if } k = j \\text{ and } l = i, \\\\\n",
    "0 & \\text{otherwise},\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "all but one term is canceled, thus yielding\n",
    "\n",
    "$$\n",
    "\\frac{dl}{dx_{i, j}} = \\frac{dl}{dx_{j, i}^T}.\n",
    "$$\n",
    "\n",
    "In other words, \n",
    "\n",
    "$$\n",
    "\\frac{dl}{dX} = \\bigg( \\frac{dl}{dX^T} \\bigg)^T.\n",
    "$$\n",
    "\n",
    "Let's do this. First, the backwards function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transpose(tensor, local_grad, prev):\n",
    "    \"\"\"\n",
    "    Transposing the backwards gradient.\n",
    "    \"\"\"  \n",
    "    \n",
    "    return tensor.backwards_grad.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the transposition. We'll adapt the NumPy interface by implementing the `Tensor.T` method, disguised as an attribute by the `@property` decorator. (`@property` is a simple way to implement computed attributes. {ref}`Check out the official documentation <https://docs.python.org/3/library/functions.html#property>` if you are not familiar with it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(Tensor):\n",
    "    # ...\n",
    "\n",
    "    @property\n",
    "    def T(self):\n",
    "        \"\"\"\n",
    "        Transpose of tensors.\n",
    "        \"\"\"\n",
    "        \n",
    "        return Tensor(\n",
    "            value=self.value.T,\n",
    "            prevs=[\n",
    "                Edge(\n",
    "                    prev=self,\n",
    "                    backward_fn=_transpose,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(As the `_transpose` backward function doesn't use the local gradient, we do not specify it.)\n",
    "\n",
    "Let's do a test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Tensor(np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[1, 4],\n",
       "       [2, 5],\n",
       "       [3, 6]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.backwards_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! This was quite a step in complexity, but taking it increased our skill level in navigating linear algebraic expressions without getting lost in the indices. (I hope.) This'll be useful to us in the next, where we tackle reshaping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping\n",
    "\n",
    "At this point, we have to dive deep into how NumPy arrays are stored behind the scenes. Consider the following array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [0, 1, 2, 3],\n",
    "    [4, 5, 6, 7],\n",
    "    [8, 9, 10, 11]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide the best performance, arrays are stored in a contiguous block of memory; that is, the values are stored next to each other. We can access this \"true form\" with the `ravel` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flat = X.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen before, the shape of the array is stored in the `shape` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With knowing the shape, we can map two-dimensional indices into linear indices! Check out the function `linear_idx` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = X.shape\n",
    "linear_idx = lambda i, j: i*m + j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't believe me? See for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[0, 0] = 0\t  X_flat[linear_idx(0, 0)] = 0\n",
      "X[0, 1] = 1\t  X_flat[linear_idx(0, 1)] = 1\n",
      "X[0, 2] = 2\t  X_flat[linear_idx(0, 2)] = 2\n",
      "X[0, 3] = 3\t  X_flat[linear_idx(0, 3)] = 3\n",
      "X[1, 0] = 4\t  X_flat[linear_idx(1, 0)] = 4\n",
      "X[1, 1] = 5\t  X_flat[linear_idx(1, 1)] = 5\n",
      "X[1, 2] = 6\t  X_flat[linear_idx(1, 2)] = 6\n",
      "X[1, 3] = 7\t  X_flat[linear_idx(1, 3)] = 7\n",
      "X[2, 0] = 8\t  X_flat[linear_idx(2, 0)] = 8\n",
      "X[2, 1] = 9\t  X_flat[linear_idx(2, 1)] = 9\n",
      "X[2, 2] = 10\t  X_flat[linear_idx(2, 2)] = 10\n",
      "X[2, 3] = 11\t  X_flat[linear_idx(2, 3)] = 11\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "for i, j in product(range(n), range(m)):\n",
    "    print(f\"X[{i}, {j}] = {X[i, j]}\\t  X_flat[linear_idx({i}, {j})] = {X_flat[linear_idx(i, j)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a match! You have probably guessed that we can go the other way around; i.e. from linear to two-dimensional indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_flat[0] = 0\t  X[array_idx(0)] = 0\n",
      "X_flat[1] = 1\t  X[array_idx(1)] = 1\n",
      "X_flat[2] = 2\t  X[array_idx(2)] = 2\n",
      "X_flat[3] = 3\t  X[array_idx(3)] = 3\n",
      "X_flat[4] = 4\t  X[array_idx(4)] = 4\n",
      "X_flat[5] = 5\t  X[array_idx(5)] = 5\n",
      "X_flat[6] = 6\t  X[array_idx(6)] = 6\n",
      "X_flat[7] = 7\t  X[array_idx(7)] = 7\n",
      "X_flat[8] = 8\t  X[array_idx(8)] = 8\n",
      "X_flat[9] = 9\t  X[array_idx(9)] = 9\n",
      "X_flat[10] = 10\t  X[array_idx(10)] = 10\n",
      "X_flat[11] = 11\t  X[array_idx(11)] = 11\n"
     ]
    }
   ],
   "source": [
    "array_idx = lambda i: (i // m, i % m)\n",
    "\n",
    "for i in range(n*m):\n",
    "    print(f\"X_flat[{i}] = {X_flat[i]}\\t  X[array_idx({i})] = {X[array_idx(i)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NumPy, whenever you write something like `X[i, j]`, it is translated to `X_flat[linear_idx(i, j)]` in the background. (*Deep* in the background, like in the C parts of the whole shebang.) Here's a `DIYArray` class that outlines what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from math import prod\n",
    "\n",
    "\n",
    "class DIYArray:\n",
    "    def __init__(self, values: List, shape: Tuple):\n",
    "        self.values = values\n",
    "        self.shape = shape\n",
    "        self._idx_fn = self.make_idx_fn(*shape)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        linear_idx = self._idx_fn(*idx)\n",
    "        return self.values[linear_idx]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        n, m = self.shape\n",
    "        return '\\n'.join(\n",
    "            \"[\" + ',\\t'.join(map(str, self.values[i*m:(i+1)*m])) + \"]\"\n",
    "            for i in range(n)\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def make_idx_fn(cls, n, m):\n",
    "        return lambda i, j: i*m + j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon initialization, `DIYArray` takes a one-dimensional array and a shape, then constructs the indexing function from the shape. Check out how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DIYArray(\n",
    "    values=list(range(12)),\n",
    "    shape=(3, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\t1,\t2,\t3]\n",
       "[4,\t5,\t6,\t7]\n",
       "[8,\t9,\t10,\t11]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's reshape the arrays. To do that, we add the `DIYArray.reshape` method. Following the NumPy interface, we initialize a new `DIYArray` from the same values with the requested shape. This is equivalent to swapping out the indexing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIYArray(DIYArray):\n",
    "    # ...\n",
    "\n",
    "    def reshape(self, *new_shape: Tuple):\n",
    "        if prod(new_shape) != prod(self.shape):\n",
    "            raise ValueError(f\"The new shape {new_shape} is not compatible with the current shape {self.shape}\")\n",
    "\n",
    "        return DIYArray(values=self.values, shape=new_shape)\n",
    "\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DIYArray(\n",
    "    values=list(range(12)),\n",
    "    shape=(3, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\t1,\t2,\t3]\n",
       "[4,\t5,\t6,\t7]\n",
       "[8,\t9,\t10,\t11]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\t1]\n",
       "[2,\t3]\n",
       "[4,\t5]\n",
       "[6,\t7]\n",
       "[8,\t9]\n",
       "[10,\t11]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.reshape(6, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradient of reshaping\n",
    "\n",
    "Now, let's talk about math. Again, it is easier to figure it out than to write it down, but here we go. Let $ X \\in \\mathbb{R}^{n \\times m} $ be an arbitrary matrix, which we reshape to $ Y \\in \\mathbb{R}^{a \\times b} $. Using the chain rule, we get\n",
    "\n",
    "$$\n",
    "\\frac{dl}{dx_{i, j}} = \\sum_{k = 1}^{a} \\sum_{l = 1}^{b} \\frac{dl}{d y_{k, l}} \\frac{\\partial y_{k, l}}{\\partial x_{i, j}}.\n",
    "$$\n",
    "\n",
    "Think about it: each $ x_{i, j} $ is moved to a unique place in the new matrix. Thus, the local derivative $ \\frac{\\partial y_{k, l}}{\\partial x_{i, j}} $ equals to $ 1 $ if $ y_{k, l} $ corresponds to $ x_{i, j} $, and zero otherwise. Formally, we have an index transformation $ (i, j) \\mapsto (i^\\prime, j^\\prime) $, thus,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_{k, l}}{\\partial x_{i, j}} = \\begin{cases}\n",
    "1 & \\text{if } k = i^\\prime \\text{ and } l = j^\\prime, \\\\\n",
    "0 & \\text{otherwise},\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "giving that\n",
    "\n",
    "$$\n",
    "\\frac{dl}{dx_{i, j}} = \\frac{dl}{d y_{i^\\prime, j^\\prime}}.\n",
    "$$\n",
    "\n",
    "In other words, we obtain $ \\frac{dl}{dX} $ by just reshaping $ \\frac{dl}{dY} $ to match the dimensions of $ X $.\n",
    "\n",
    "To carry this out in practice, we'll create the `_reshape` function that just reshapes `backwards_grad` to the desired shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reshape(tensor, local_grad, prev):\n",
    "    \"\"\"\n",
    "    Reshapes the backwards gradient to the shape of the local gradient.\n",
    "    \"\"\"\n",
    "    return tensor.backwards_grad.reshape(prev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(Tensor):\n",
    "    # ...\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.value.shape\n",
    "    \n",
    "    @property\n",
    "    def ndim(self):\n",
    "        return self.value.ndim\n",
    "\n",
    "    def reshape(self, *args):\n",
    "        return Tensor(\n",
    "            value=self.value.reshape(*args),\n",
    "            prevs=[\n",
    "                Edge(\n",
    "                    prev=self,\n",
    "                    backward_fn=_reshape,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Tensor(np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X.reshape(6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[ 1,  2],\n",
       "       [ 3,  4],\n",
       "       [ 5,  6],\n",
       "       [ 7,  8],\n",
       "       [ 9, 10],\n",
       "       [11, 12]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the backward step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.backwards_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.backwards_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't ease up just yet; we still have operations to go. Let's dive into broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "One of the first things we learn in linear algebra is that matrices can be summed if and only if their shape is equal.\n",
    "\n",
    "I'm here to tell you: this is not exactly the case, at least not in the computational sense. Let me explain with an example.\n",
    "\n",
    "Suppose that we have a data matrix $ X \\in \\mathbb{R}^{N \\times m} $, and we want to center the data by subtracting the feature mean vector $ X_\\mu \\in \\mathbb{R}^{m} $ from each sample. Wouldn't it be awesome to write $ X - X_\\mu $ without manually stacking $ X_\\mu $ $ N $ times to match the $ N \\times m $ shape?\n",
    "\n",
    "We can do just that in NumPy. Check it out. First, here's an array `X` with shape `(5, 3)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0, 0],\n",
    "              [1, 1, 1], \n",
    "              [2, 2, 2],\n",
    "              [3, 3, 3],\n",
    "              [4, 4, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, here's a vector, represented by a one-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we execute `X + y`? Take a wild guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [1, 2, 3],\n",
       "       [2, 3, 4],\n",
       "       [3, 4, 5],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the magic of broadcasting, we have added the vector `y` to every row of `X` with a simple `X + y` without any additional steps! In the background, NumPy broadcasted `y` to match the shape of `X` by stacking it five times along the vertical axis.\n",
    "\n",
    "To see what happens, we can manually broadcast `y` with NumPy's built-in `np.broadcast_to`, which takes an array and a shape to broadcast. Check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [0, 1, 2],\n",
       "       [0, 1, 2],\n",
       "       [0, 1, 2],\n",
       "       [0, 1, 2]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast_to(y, X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing `X + y` is equivalent to `X + np.broadcast_to(y, X.shape)`, but the former is much simpler.\n",
    "\n",
    "Can we do the same, but with columns? Yes. Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([0, 1, 2, 3, 4]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [2, 2, 2],\n",
       "       [4, 4, 4],\n",
       "       [6, 6, 6],\n",
       "       [8, 8, 8]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X + z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this time, the shape of the mismatched matrix is `(5, 1)`, not just plainly `(5,)`. The latter one wouldn't work.\n",
    "\n",
    "Alright. One more example before we discuss the broadcasting rules of NumPy. What happens if we add a column vector and a row vector together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0, 1, 2]).reshape(3, 1)   # column vector\n",
    "b = np.array([0, 1, 2, 3]).reshape(1, 4)   # row vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3],\n",
       "       [1, 2, 3, 4],\n",
       "       [2, 3, 4, 5]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a matrix! NumPy automagically figures out the common shape, repeats the arrays along the appropriate dimensions, and adds them together pointwise.\n",
    "\n",
    "### How broadcasting works\n",
    "\n",
    "Broadcasting can feel like magic, but it is just a sequence of simple steps. Say, we have an array `X` of shape `(3, 4)`, and an array `Y` of shape `(2, 1, 4)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ones((3, 4))\n",
    "Y = np.ones((2, 1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, NumPy checks if the number of dimensions matches. If not, it left-pads the shape of the lower-dimensional array with ones. In this case, `X` is padded to `(1, 1, 4)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_shapes(X, Y):\n",
    "    diff = len(Y.shape) - len(X.shape)\n",
    "    \n",
    "    if diff > 0:\n",
    "        X = X.reshape((1,) * diff + X.shape)\n",
    "    elif diff < 0:\n",
    "        Y = Y.reshape((1,) * -diff + Y.shape)\n",
    "    \n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = pad_shapes(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X.shape = (1, 3, 4), Y.shape = (2, 1, 4)'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"X.shape = {X.shape}, Y.shape = {Y.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, it checks if the padded shapes are compatible; that is, it compares the dimensions elementwise. It is easier to write some pseudocode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_shapes(X_shape, Y_shape):\n",
    "    return all(d1 == d2 or d1 == 1 or d2 == 1 for d1, d2 in zip(X_shape, Y_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_shapes(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, NumPy finds the common dimension by taking the elementwise maximum of two shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcasted_shape(X_shape, Y_shape):\n",
    "    return tuple(max(i, j) for i, j in zip(X_shape, Y_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_shape = broadcasted_shape(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The broadcasted shape of X and Y is (2, 3, 4).'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"The broadcasted shape of X and Y is {bs_shape}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the broadcasted shape is ready, we can copy the arrays along the non-matching dimensions to finish the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_along_axis(X, target_shape):\n",
    "    repeats = tuple(t // s if t > s else 1 for s, t in zip(X.shape, target_shape))\n",
    "    return np.tile(X, repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bc = repeat_along_axis(X, bs_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put these together into a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast(X, Y):\n",
    "    X, Y = pad_shapes(X, Y)\n",
    "\n",
    "    if not check_shapes(X.shape, Y.shape):\n",
    "        return ValueError(\"The shapes are not compatible.\")\n",
    "    \n",
    "    bs_shape = broadcasted_shape(X.shape, Y.shape)\n",
    "\n",
    "    return repeat_along_axis(X, bs_shape), repeat_along_axis(Y, bs_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bc, Y_bc = broadcast(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_bc.shape = (2, 3, 4), Y_bc.shape = (2, 3, 4)'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"X_bc.shape = {X_bc.shape}, Y_bc.shape = {Y_bc.shape}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit our earlier examples and see if our custom broadcasting function works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0, 0],\n",
    "              [1, 1, 1], \n",
    "              [2, 2, 2],\n",
    "              [3, 3, 3],\n",
    "              [4, 4, 4]])\n",
    "\n",
    "y = np.array([0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bc, y_bc = broadcast(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [0, 1, 2],\n",
       "       [0, 1, 2],\n",
       "       [0, 1, 2],\n",
       "       [0, 1, 2]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the (row) vector `y` was copied vertically five times. So far, so good. What about the column vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([0, 1, 2, 3, 4]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bc, z_bc = broadcast(X, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [1, 1, 1],\n",
       "       [2, 2, 2],\n",
       "       [3, 3, 3],\n",
       "       [4, 4, 4]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! One more example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0, 1, 2]).reshape(3, 1)   # column vector\n",
    "b = np.array([0, 1, 2, 3]).reshape(1, 4)   # row vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_bc, b_bc = broadcast(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [1, 1, 1, 1],\n",
       "       [2, 2, 2, 2]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3],\n",
       "       [0, 1, 2, 3],\n",
       "       [0, 1, 2, 3]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a + b == a_bc + b_bc).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, broadcasting is accessible via NumPy functions. Our custom function is there for pedagogical reasons; we'll use `np.broadcast_to`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [1, 1, 1, 1],\n",
       "       [2, 2, 2, 2]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast_to(a, (3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3],\n",
       "       [0, 1, 2, 3],\n",
       "       [0, 1, 2, 3]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast_to(b, (3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradient of broadcasting\n",
    "\n",
    "Now that we understand what broadcasting is and how it's done, the time has come to compute its backward derivative. Let's see a special case: a vector $ \\mathbf{x} \\in \\mathbb{R}^{4} $, broadcasted to the matrix $ Y \\in \\mathbb{R}^{3 \\times 4} $, that is,\n",
    "\n",
    "$$\n",
    "(x_1, x_2, x_3, x_4) \\mapsto \\begin{bmatrix}\n",
    "y_{1, 1} & y_{1, 2} & y_{1, 3} & y_{1, 4} \\\\\n",
    "y_{2, 1} & y_{2, 2} & y_{2, 3} & y_{2, 4} \\\\\n",
    "y_{3, 1} & y_{3, 2} & y_{3, 3} & y_{3, 4} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "What is $ Y $? Broadcasting is copying along the new dimensions; thus,\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_{1, 1} & y_{1, 2} & y_{1, 3} & y_{1, 4} \\\\\n",
    "y_{2, 1} & y_{2, 2} & y_{2, 3} & y_{2, 4} \\\\\n",
    "y_{3, 1} & y_{3, 2} & y_{3, 3} & y_{3, 4} \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "x_1 & x_2 & x_3 & x_4 \\\\\n",
    "x_1 & x_2 & x_3 & x_4 \\\\\n",
    "x_1 & x_2 & x_3 & x_4 \\\\\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "or in other words, $ y_{i, j} = x_j $.\n",
    "\n",
    "So far, so good. Now, the chain rule says\n",
    "\n",
    "$$\n",
    "\\frac{dl}{dx_i} = \\sum_{k = 1}^{3} \\sum_{l = 1}^{4} \\frac{dl}{d y_{k, l}} \\frac{\\partial y_{k, l}}{\\partial x_i}.\n",
    "$$\n",
    "\n",
    "Per usual, the key is figuring out $ \\frac{\\partial y_{k, l}}{\\partial x_i} $. As $ y_{i, j} = x_j $, we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_{k, l}}{\\partial x_i} = \\begin{cases}\n",
    "1 & \\text{if } l = i, \\\\\n",
    "0 & \\text{otherwise},\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "thus, continuing what the chain rule gives,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{dl}{dx_i} &= \\sum_{k = 1}^{3} \\sum_{l = 1}^{4} \\frac{dl}{d y_{k, l}} \\frac{\\partial y_{k, l}}{\\partial x_i} \\\\\n",
    "&= \\sum_{k = 1}^{3} \\frac{dl}{d y_{k, l}},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "that is, we sum $ \\frac{dl}{dY} $ along the new dimension! In NumPy, this is equivalent to\n",
    "\n",
    "```\n",
    "dx = np.sum(dY, axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, we can conjecture that in the general case, we'll sum along the new dimensions. I am here to tell you: this is true, but we will not prove it here. Showing that involves a bunch of convoluted formalism, and you already understand what is essential. The rest is just mathematical busywork.\n",
    "\n",
    "Let's add broadcasting to `Tensor`! For broadcasting the underlying NumPy arrays, we'll just use `np.broadcast_to`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(Tensor):\n",
    "    # ...\n",
    "\n",
    "    def broadcast_to(self, shape):\n",
    "        return Tensor(\n",
    "            value=np.broadcast_to(self.value, shape),\n",
    "            prevs=[\n",
    "                Edge(\n",
    "                    prev=self,\n",
    "                    local_grad=None,\n",
    "                    backward_fn=None,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the backward function? The \"summing along the new dimensions\" backward function is the special case of the *reduce* algorithm. Without further ado, here's the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reduce(tensor, local_grad, prev):\n",
    "    \"\"\"\n",
    "    Sums the backwards gradient along axes to match the shape of the\n",
    "    local gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    # checking if reduction is possible\n",
    "    if prev.ndim > tensor.backwards_grad.ndim:\n",
    "        raise ValueError(\n",
    "            f\"Shapes {tensor.backwards_grad.shape} and {prev.shape} are not compatible.\"\n",
    "        )\n",
    "\n",
    "    # padding the shape of the backwards gradient with ones\n",
    "    prev_shape = (1,) * (tensor.backwards_grad.ndim - prev.ndim) + prev.shape\n",
    "\n",
    "    # find the axes to sum along\n",
    "    axes_to_sum = [\n",
    "        i\n",
    "        for i, (bg, lg) in enumerate(zip(tensor.backwards_grad.shape, prev_shape))\n",
    "        if lg == 1 and bg != 1\n",
    "    ]\n",
    "\n",
    "    return np.sum(\n",
    "        tensor.backwards_grad, axis=tuple(axes_to_sum), keepdims=True\n",
    "    ).reshape(prev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through it. To find the axes to sum along, we use our knowledge of broadcasting rules:\n",
    "* we left-pad the shape of the tensor's backward gradient with ones to match the dimensions of `prev`,\n",
    "* walk through the shapes one by one to find the axes to sum along,\n",
    "* then perform the summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(Tensor):\n",
    "    # ...\n",
    "\n",
    "    def broadcast_to(self, shape):\n",
    "        return Tensor(\n",
    "            value=np.broadcast_to(self.value, shape),\n",
    "            prevs=[\n",
    "                Edge(\n",
    "                    prev=self,\n",
    "                    backward_fn=_reduce,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor(np.array([0, 1, 2]).reshape(3, 1))   # column vector\n",
    "b = Tensor(np.array([0, 1, 2, 3]).reshape(1, 4))   # row vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = a.broadcast_to((3, 4))\n",
    "Y = b.broadcast_to((3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[0, 0, 0, 0],\n",
       "       [1, 1, 1, 1],\n",
       "       [2, 2, 2, 2]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[0, 1, 2, 3],\n",
       "       [0, 1, 2, 3],\n",
       "       [0, 1, 2, 3]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works as intended. What about the backward gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.backward()\n",
    "Y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.backwards_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4],\n",
       "       [4],\n",
       "       [4]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.backwards_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.backwards_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 3, 3]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.backwards_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, working as intended. We are almost at the finish line with broadcasting. There is only one more feature left: adding automatic broadcasting for operations like addition and elementwise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic operations revisited\n",
    "\n",
    "Let's revisit our previous example of a row and column vector to see what happens when we add them together as tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor(np.array([0, 1, 2]).reshape(3, 1))   # column vector\n",
    "b = Tensor(np.array([0, 1, 2, 3]).reshape(1, 4))   # row vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[0, 1, 2, 3],\n",
       "       [1, 2, 3, 4],\n",
       "       [2, 3, 4, 5]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. What's the backward gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1]], dtype=object)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.backwards_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh. Something is wrong. Recall that the backward gradient's shape must match the tensor's shape. This is definitely not the case. Can you guess what went wrong?\n",
    "\n",
    "I won't leave you in the dark for long: the broadcasting. As NumPy performs it automatically, it is not part of our computational graph; hence, it is not taken into account during backpropagation. If we broadcast manually, this issue is solved. Check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor(np.array([0, 1, 2]).reshape(3, 1))   # column vector\n",
    "b = Tensor(np.array([0, 1, 2, 3]).reshape(1, 4))   # row vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.broadcast_to((3, 4)) + b.broadcast_to((3, 4))    # addition with manual broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4],\n",
       "       [4],\n",
       "       [4]], dtype=object)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.backwards_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 3, 3]], dtype=object)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.backwards_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! \n",
    "\n",
    "Let's add this feature to `Tensor`'s corresponding operators to make it broadcast automatically. Here's a simple function named `precast` that explicitly performs broadcasting if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precast(x: Tensor, y: Tensor):\n",
    "    if x.shape != y.shape:\n",
    "        bs_shape = broadcasted_shape(x.shape, y.shape)\n",
    "        x, y = x.broadcast_to(bs_shape), y.broadcast_to(bs_shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(Tensor):\n",
    "    # ...\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Pointwise addition of tensors.\n",
    "        \"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            other = Tensor(other)\n",
    "\n",
    "        self, other = precast(self, other)\n",
    "        \n",
    "        return Tensor(\n",
    "            value=self.value + other.value,\n",
    "            prevs=[\n",
    "                Edge(prev=self, local_grad=np.ones_like(self), backward_fn=_pointwise),\n",
    "                Edge(\n",
    "                    prev=other,\n",
    "                    local_grad=np.ones_like(other),\n",
    "                    backward_fn=_pointwise,\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a similar broadcast-check to elementwise multiplication (`__mul__`), exponentiation (`__pow__`), division (`__div__`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor(np.array([0, 1, 2]).reshape(3, 1))   # column vector\n",
    "b = Tensor(np.array([0, 1, 2, 3]).reshape(1, 4))   # row vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b   # addition with automatic broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4],\n",
       "       [4],\n",
       "       [4]], dtype=object)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.backwards_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 3, 3]], dtype=object)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.backwards_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that, our work with broadcasting is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction operators\n",
    "\n",
    "To *sum up* the previous section, broadcasting blows up the tensor by adding new dimensions and copy-pasting its contents along the new axes. The gradient of broadcasting is reduction via summation.\n",
    "\n",
    "This gives us an idea of what's missing from our `Tensor` class! So far, we have yet to find a convenient method to compute the mean and sum of a tensor. This is a convenient feature of NumPy, so we should also have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 1, 2, 3],\n",
    "              [1, 2, 3, 4],\n",
    "              [2, 3, 4, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(30)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.5)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even specify the axes to sum along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  6,  9, 12])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 10, 14])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we can choose to preserve the dimensions of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6],\n",
       "       [10],\n",
       "       [14]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement this in our `Tensor`! Compared to monsters like broadcasting or convolution, summation is a cakewalk. We'll do two special cases first, then generalize.  \n",
    "\n",
    "First, we'll sum up all the elements. For any matrix $ X \\in \\mathbb{R}^{n \\times m} $, its sum is defined by\n",
    "\n",
    "$$\n",
    "s = \\sum_{k = 1}^{n} \\sum_{l = 1}^{m} x_{i, j}.\n",
    "$$\n",
    "\n",
    "As usual, we invoke the chain rule to find that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{dl}{dx_{i, j}} &= \\frac{dl}{ds} \\frac{\\partial s}{\\partial x_{i, j}} \\\\\n",
    "&= \\frac{dl}{ds},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "that is, we obtain the global gradient $ \\frac{dl}{dX} \\in \\mathbb{R}^{n \\times n} $ by simply blowing up the constant $ s \\in \\mathbb{R} $:\n",
    "\n",
    "$$\n",
    "\\frac{dl}{dX} = \\begin{bmatrix}\n",
    "\\frac{dl}{ds} & \\frac{dl}{ds} & \\dots & \\frac{dl}{ds} \\\\\n",
    "\\frac{dl}{ds} & \\frac{dl}{ds} & \\dots & \\frac{dl}{ds} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{dl}{ds} & \\frac{dl}{ds} & \\dots & \\frac{dl}{ds} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "In a sense, we broadcast the $ 1 \\times 1 $ matrix that is $ \\frac{dl}{ds} $ into a $ n \\times m $ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is broadcasting the gradient of summation? Let's see the second special case, where we sum the matrix $ X \\in \\mathbb{R}^{n \\times m} $ along the zeroth axis. (That is, we sum up its columns.) This way, the sum is given by the row vector\n",
    "\n",
    "$$\n",
    "\\mathbf{s} = \\bigg(\\sum_{i = 1}^{n} x_{i, 1}, \\sum_{i = 1}^{n} x_{i, 2}, \\dots \\sum_{i = 1}^{n} x_{i, m}\\bigg) \\in \\mathbb{R}^m.\n",
    "$$\n",
    "\n",
    "(Note that in practice, the shape depends on the value of `keepdims`.)\n",
    "\n",
    "With the notation $ s_j = \\sum_{i = 1}^{n} x_{i, j} $, the chain rule gives\n",
    "\n",
    "$$\n",
    "\\frac{dl}{dx_{i, j}} = \\sum_{k=1}^{m} \\frac{dl}{ds_k} \\frac{\\partial s_k}{\\partial x_{i, j}},\n",
    "$$\n",
    "\n",
    "so let's check $ \\frac{\\partial s_k}{\\partial x_{i, j}} $. As $ s_k $ is the sum of the $ k $-th column, $ \\frac{\\partial s_k}{\\partial x_{i, j}} $ must be zero if $ k \\neq j $. As each $ x_{i, j} $ appears exactly once in the column sum $ s_j $, $ \\frac{\\partial s_j}{\\partial x_{i, j}} = 1 $. Thus,\n",
    "\n",
    "$$\n",
    "\\frac{dl}{dx_{i, j}} = \\frac{dl}{ds_j},\n",
    "$$\n",
    "\n",
    "or in other words, we broadcast the vector $ \\frac{dl}{d\\mathbf{s}} $ to match the shape of $ X $ to obtain $ \\frac{dl}{dX} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the gradient function of summation is broadcasting. However, there's an issue in practice. Let's jump a dimension, and consider an array of shape `(3, 4, 5)`. Check what happens when we sum along the middle axis, then attempt to broadcast the result back to shape `(3, 4, 5)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp = (3, 4, 5)\n",
    "X = np.ones(shp)\n",
    "S = X.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to insert an axis between the two dimensions of `S` and repeat it four times. Can broadcasting do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with remapped shapes [original->remapped]: (3,5)  and requested shape (3,4,5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/mlfz/lib/python3.12/site-packages/numpy/lib/_stride_tricks_impl.py:424\u001b[0m, in \u001b[0;36mbroadcast_to\u001b[0;34m(array, shape, subok)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_broadcast_to_dispatcher, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbroadcast_to\u001b[39m(array, shape, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    379\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Broadcast an array to a new shape.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m           [1, 2, 3]])\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreadonly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/mlfz/lib/python3.12/site-packages/numpy/lib/_stride_tricks_impl.py:359\u001b[0m, in \u001b[0;36m_broadcast_to\u001b[0;34m(array, shape, subok, readonly)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall elements of broadcast shape must be non-\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    357\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    358\u001b[0m extras \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 359\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnditer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmulti_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrefs_ok\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzerosize_ok\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mextras\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_flags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreadonly\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitershape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m it:\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;66;03m# never really has writebackifcopy semantics\u001b[39;00m\n\u001b[1;32m    364\u001b[0m     broadcast \u001b[38;5;241m=\u001b[39m it\u001b[38;5;241m.\u001b[39mitviews[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with remapped shapes [original->remapped]: (3,5)  and requested shape (3,4,5)"
     ]
    }
   ],
   "source": [
    "np.broadcast_to(S, shp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, it cannot. To perform what we want, we need to turn `S` into an array of shape `(3, 1, 5)` to obtain broadcastable arrays, then do the broadcasting. This operation is called *tiling*.\n",
    "\n",
    "First, here's a function that aligns the tuples and fills the missing values with ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tuple(x: Tuple, align_to: Tuple):\n",
    "    tpl_list = list(x)\n",
    "    return tuple(\n",
    "        tpl_list.pop(tpl_list.index(val)) if val in tpl_list else 1 for val in align_to\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a few examples to understand what `align_tuple` is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 5)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_tuple(x=(3, 5), align_to=(3, 4, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 5, 1, 1, 8, 1)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_tuple(x=(3, 5, 8), align_to=(3, 4, 5, 6, 7, 8, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in hand, we can finish the tiling by doing the broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tile(tensor, local_grad, prev):\n",
    "    \"\"\"\n",
    "    Broadcasts the backwards gradient to match the backwards gradient\n",
    "    of the previous tensor.\n",
    "    \"\"\"\n",
    "    backwards_grad_new_shape = align_tuple(tensor.shape, prev.shape)\n",
    "    backwards_grad = tensor.backwards_grad.reshape(backwards_grad_new_shape)\n",
    "    return np.broadcast_to(backwards_grad, prev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here's the `Tensor.sum` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(Tensor):\n",
    "    # ...\n",
    "\n",
    "    def sum(self, axis=None, keepdims=False):\n",
    "        return Tensor(\n",
    "            value=self.value.sum(axis=axis, keepdims=keepdims),\n",
    "            prevs=[\n",
    "                Edge(\n",
    "                    prev=self,\n",
    "                    backward_fn=_tile,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Tensor(np.array([[0, 1, 2, 3],\n",
    "                     [1, 2, 3, 4],\n",
    "                     [2, 3, 4, 5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = X.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[ 3,  6,  9, 12]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.backwards_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing correctness with finite differences\n",
    "\n",
    "Phew. This chapter was extra long, and there's still one burning question that remains.\n",
    "\n",
    "Is our implementation correct? Are we free to use it or throw it all out and start over? Let's check it with finite differences. We have used them before for {ref}`verifying the backpropagation for scalars <section:scalar/backward-pass-practice/finite-diff>`, but let's recall to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_diff(f, x, h=1e-8):\n",
    "    result = (f(x + h) - f(x - h))/(2*h)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test with the `Tensor` from `mlfz`. This is just to access the full functionality of tensors without having to add hundreds of lines of code in this notebook. This chapter is long enough as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlfz.nn import Tensor as mlfz_Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a nontrivial function, let's use the hyperbolic tangent. Although we haven't explicitly tensorized functions, you should be able to do it yourself. As pointwise function application is a simple operation, give it a go by yourself.\n",
    "\n",
    "Here's the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tanh_prime(x: np.ndarray):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "\n",
    "def tanh(x: mlfz_Tensor):\n",
    "    return mlfz_Tensor(\n",
    "        value=np.tanh(x.value),\n",
    "        prevs=[Edge(prev=x, local_grad=_tanh_prime(x.value), backward_fn=_pointwise)],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a more complicated function that acts on `Tensor` instances. We'll use this to verify the correctness of our vectorized backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return tanh(3 * x - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mlfz_Tensor(np.random.rand(3, 4))\n",
    "Y = f(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the derivatives with finite differences and backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dY_fd = finite_diff(f, X)\n",
    "dY = X.backwards_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[2.49726038, 2.89647918, 2.31822262, 0.35300437],\n",
       "       [1.8843105 , 2.44953318, 2.81431392, 1.29562128],\n",
       "       [2.47743078, 1.58617426, 2.99970976, 1.41936041]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.49726036, 2.89647918, 2.31822262, 0.35300437],\n",
       "       [1.8843105 , 2.44953316, 2.8143139 , 1.29562128],\n",
       "       [2.47743077, 1.58617426, 2.99970976, 1.41936041]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They seem to be a pretty good match, verified even by the ever-underrated `np.allclose` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(dY_fd, X.backwards_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlfz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
