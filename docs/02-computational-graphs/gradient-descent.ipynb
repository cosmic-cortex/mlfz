{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing with gradient descent\n",
    "\n",
    "Our neural network framework is almost complete. The `Scalar` class has almost everything we need, the one thing left to do is to implement gradient descent.\n",
    "\n",
    "Let's recap: suppose that we want to find the minima of the function $ f(x) $. Analytic solutions are rarely possible, so quite often the best option is an iterative method called gradient descent. According to theory, if the starting point $ x_0 $ and the learning rate $ h $ is <strike>luckily</strike> properly selected, the sequence\n",
    "\n",
    "$$\n",
    "x_{n + 1} = x_n - h f^\\prime(x_n)\n",
    "$$ (eq:computational-graphs/gradient-descent/gradient-update)\n",
    "\n",
    "converges to a local minimum. I know, it feels like black magic, but it works.\n",
    "\n",
    "So far, here's our training loop.\n",
    "\n",
    "```\n",
    "# -- inputs --\n",
    "# model: the machine learning model\n",
    "# xs: training data\n",
    "# ys: the ground truth\n",
    "# n_epochs: the number of iterations\n",
    "# lr: learning rate\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    ys_pred = [model(x) for x in xs]\n",
    "    loss = loss_function(ys, ys_pred)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.parameters:\n",
    "        p.gradient_update(lr)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's only one thing missing: the gradient update. Based on {eq}`eq:computational-graphs/gradient-descent/gradient-update`, it's quite simple to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
